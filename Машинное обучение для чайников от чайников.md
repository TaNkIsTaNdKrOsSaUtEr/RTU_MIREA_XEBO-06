**Машинное обучение для чайников от чайников (для химиков)**

**Что это за зверь и с чем его едят?**

Машинное обучение (ML) — это когда ты заставляешь компьютер угадывать ответы, как студент на экзамене, который прочитал только первую страницу учебника. Главное — накормить его правильными данными и надеяться, что он не переобуется на ходу.

##     **Содержание**
###      Основы
###      Pandas
###      Numpy
###      Sklearn
###      Matplotlib
###      Rdkit

# Основы

**Основные ингредиенты ML:**

1. **Данные** — твои реактивы. Без них нихуя не получится
2. **Признаки (т.н. features)** — характеристики молекул (масса, заряд, полярность и т.д.)
3. **Целевая переменная** — что мы хотим предсказать (активность, растворимость, токсичность)
4. **Модель** — собсна сама штука, которая обрабатывает все эти данные, признаки и т.п.

---

**Библиотеки — наши инстурменты**

```python
# Стандартный набор химика-MLщика
import pandas as pd  # Для табличек, типа Excel 
import numpy as np   # Для циферок и матриц
from sklearn import *  # Главная ML-библиотека
import matplotlib.pyplot as plt  # Чтобы рисовать красивые графики
from rdkit import Chem  # Химическая хуйня
```

**Установка всего этого добра:**
```bash
pip install pandas numpy scikit-learn matplotlib rdkit-pypi
```

---

**Подготовка данных (спиздить реактивы у соседей, занять место под вытяжкой и т.п.)**

**Загрузка данных:**
```python
# Вариант 1: из CSV файла
data = pd.read_csv('my_chemical_data.csv')

# Вариант 2: сгенерировать самому (когда лень искать реальные данные)
data = pd.DataFrame({
    'molecular_weight': np.random.normal(200, 50, 100),
    'logP': np.random.normal(2, 1, 100),
    'solubility': np.random.normal(-3, 1, 100)
})
```

**Предобработка (самая скучная часть):**
```python
# Удаляем строки с пропущенными значениями
data = data.dropna()

# Разделяем на признаки (X) и целевую переменную (y)
X = data[['molecular_weight', 'logP']]
y = data['solubility']

# Делим на обучающую и тестовую выборки
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

---

**Модели**

**Линейная регрессия** — как предсказать цену бухла по градусам:
```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)  # Обучаем модель
predictions = model.predict(X_test)  # Предсказываем
```

**Случайный лес** — когда не знаешь, какая модель лучше, но хочешь чтобы работало:
```python
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

**Метод k-ближайших соседей** — смотрим на похожих соседей:
```python
from sklearn.neighbors import KNeighborsRegressor

model = KNeighborsRegressor(n_neighbors=5)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

---

**RDKit — химическая хуйня**

**Чтение молекул из SMILES:**
```python
from rdkit import Chem
from rdkit.Chem import Draw

# Превращаем текстовые описания в молекулы
caffeine_smiles = 'CN1C=NC2=C1C(=O)N(C(=O)N2C)C'
caffeine = Chem.MolFromSmiles(caffeine_smiles)

# Рисуем молекулу (вау-эффект обеспечен)
img = Draw.MolToImage(caffeine)
img.save('caffeine.png')
```

**Вычисление молекулярных дескрипторов:**
```python
from rdkit.Chem import Descriptors

# Считаем молекулярную массу и логP
mw = Descriptors.MolWt(caffeine)
logp = Descriptors.MolLogP(caffeine)

print(f"Масса кофеина: {mw:.2f}")
print(f"LogP кофеина: {logp:.2f}")
```

**Молекулярные отпечатки:**
```python
from rdkit.Chem import AllChem

# Создаем отпечатки для ML
fp = AllChem.GetMorganFingerprintAsBitVect(caffeine, radius=2, nBits=1024)
fp_array = np.array(fp)  # Превращаем в массив для ML
```

---

**Оценка моделей — ставим оценки**

```python
from sklearn.metrics import mean_squared_error, r2_score

# Среднеквадратичная ошибка (чем меньше, тем лучше)
mse = mean_squared_error(y_test, predictions)

# R² score (чем ближе к 1, тем лучше)
r2 = r2_score(y_test, predictions)

print(f"MSE: {mse:.3f}")
print(f"R²: {r2:.3f}")

# Если R² отрицательный — твоя модель хуже, чем просто предсказывать среднее значение
# Поздравляю, ты достиг дна ML!
```

---

**Классификация — когда нужно просто "да/нет"**

**Логистическая регрессия** — для бинарной классификации (токсично/нет):
```python
from sklearn.linear_model import LogisticRegression

# Предположим, у нас есть бинарный таргет
y_binary = (y > y.median()).astype(int)  # Выше медиана = 1, ниже = 0

model = LogisticRegression()
model.fit(X_train, y_binary)
predictions = model.predict(X_test)

# Оценка точности
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, predictions)
print(f"Точность: {accuracy:.2f}")
```

---

**Перекрестная проверка — чтобы не обманывать себя**

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5, scoring='r2')
print(f"R² на кросс-валидации: {scores.mean():.3f} ± {scores.std():.3f}")
```

---

**Гиперпараметры — крутилки-настройки**

**GridSearchCV** — перебираем все возможные варианты:
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20]
}

grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5)
grid_search.fit(X_train, y_train)

print(f"Лучшие параметры: {grid_search.best_params_}")
```

---

**Кластеризация — ищем похожие молекулы**

**K-means** — группируем молекулы по похожести:
```python
from sklearn.cluster import KMeans

# Предположим, у нас есть матрица отпечатков пальцев
kmeans = KMeans(n_clusters=5, random_state=42)
clusters = kmeans.fit_predict(fingerprint_matrix)

print(f"Молекулы разбиты на {len(np.unique(clusters))} кластера")
```

---

**Нейронные сети — когда простые модели уже не кайф**

```python
from sklearn.neural_network import MLPRegressor

model = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000)
model.fit(X_train, y_train)

# Внимание: может потребоваться масштабирование данных!
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

---

**Типичные грабли, на которые ты наступишь**

1. **Утечка данных**
```python
# НЕПРАВИЛЬНО:
scaler.fit(X)  # Обучаем на всех данных
X_scaled = scaler.transform(X)

# ПРАВИЛЬНО:
scaler.fit(X_train)  # Обучаем только на тренировочных данных
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

2. **Переобучение** — когда модель запоминает ответы, но не понимает сути:
```python
# Симптомы: accuracy на train = 99%, на test = 50%
# Лечение: regularization, упрощение модели, больше данных
```

3. **Нормализация** — когда признаки в разных масштабах:
```python
# Молекулярная масса: 100-500
# LogP: -2 до 8
# Без нормализации модель будет обращать внимание только на массу!
```

---

**Полезные советы**

- **Начинайте с простых моделей** — линейная регрессия и случайный лес
- **Визуализируйте всё** — если не видно закономерность глазом, модель её тоже не найдет
- **Не верьте слепо метрикам** — смотрите на реальные предсказания
- **Вы умнее куска кода** — если модель предсказывает бред, скорее всего это бред

**Пример полного пайплайна:**
```python
# 1. Загрузка данных
data = pd.read_csv('chemical_data.csv')

# 2. Предобработка
X = data[['feature1', 'feature2', 'feature3']]
y = data['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 3. Масштабирование
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. Обучение модели
model = RandomForestRegressor(n_estimators=100)
model.fit(X_train_scaled, y_train)

# 5. Предсказание и оценка
predictions = model.predict(X_test_scaled)
print(f"R²: {r2_score(y_test, predictions):.3f}")
```

---

**Когда всё идет по пизде**

Если твоя модель работает хуже, чем предсказание среднего значения:

1. **Проверь данные** — нет ли пропусков, выбросов
2. **Убери коррелирующие признаки** 
3. **Попробуй другую модель**
4. **Собери больше данных**
5. **Пересмотри свой жизненный выбор, тебе точно нужна норм оценка по СИИБД?**

**Помни:** даже плохая ML-модель лучше, чем гадание на таро. Ну, почти всегда.

*P.S. Если что-то не работает — попробуй выключить и включить. Если не поможет — иди пить пиво.*

# Pandas

Pandas - Excel по питоновски

### **Шаг 1: Создание DataFrame (таблички)**
```python
# Способ 1: Из словаря (самый простой)
data = {
    'Имя': ['Вася', 'Петя', 'Маша', 'Даша'],
    'Возраст': [25, 30, 35, 28],
    'Зарплата': [50000, 60000, 70000, 55000]
}

df = pd.DataFrame(data)
print(df)
```
```
   Имя  Возраст  Зарплата
0  Вася      25     50000
1  Петя      30     60000  
2  Маша      35     70000
3  Даша      28     55000
```

---

### **Шаг 2: Чтение данных из файла**
```python
# Читаем CSV файл Если файл в той же папке, что и твой код
df = pd.read_csv('данные_химия.csv')

# Посмотреть первые 5 строк
print(df.head())
```

---

### **Шаг 3: Основные команды для просмотра данных**
```python
# Первые 3 строки
df.head(3)

# Последние 3 строки  
df.tail(3)

# Размер таблицы (строки, столбцы)
print(df.shape)

# Общая информация о данных
df.info()

# Статистика по числовым столбцам
df.describe()
```

---

### **Шаг 4: Выбор данных (самое важное!)**
```python
# Выбор одного столбца
ages = df['Возраст']
print(ages)

# Выбор нескольких столбцов
subset = df[['Имя', 'Зарплата']]
print(subset)

# Выбор строк по индексу
row_2 = df.iloc[2]  # Третья строка (индексы с 0)
print(row_2)

# Выбор диапазона строк
first_3 = df.iloc[0:3]  # Строки 0, 1, 2
print(first_3)
```

---

### **Шаг 5: Фильтрация данных**
```python
# Все, кто старше 28 лет
old_people = df[df['Возраст'] > 28]
print(old_people)

# Люди с зарплатой от 50к до 65к
middle_salary = df[(df['Зарплата'] >= 50000) & (df['Зарплата'] <= 65000)]
print(middle_salary)

# Фильтр по тексту (все Васи)
vasyas = df[df['Имя'] == 'Вася']
print(vasyas)
```

---

### **Шаг 6: Добавление и удаление столбцов**
```python
# Добавляем новый столбец
df['Премия'] = df['Зарплата'] * 0.1  # 10% от зарплаты
print(df)

# Удаляем столбец
df = df.drop('Премия', axis=1)  # axis=1 значит "столбец"
print(df)

# Можно удалить несколько
df = df.drop(['Столбец1', 'Столбец2'], axis=1)
```

---

### **Шаг 7: Работа с пропущенными значениями**
```python
# Проверяем есть ли пропуски
print(df.isnull().sum())

# Заполняем пропуски нулями
df_filled = df.fillna(0)

# Удаляем строки с пропусками
df_clean = df.dropna()
```

---

### **Шаг 8: Сортировка**
```python
# Сортировка по возрасту (по возрастанию)
df_sorted = df.sort_values('Возраст')
print(df_sorted)

# Сортировка по зарплате (по убыванию)
df_sorted_desc = df.sort_values('Зарплата', ascending=False)
print(df_sorted_desc)
```

---

### **Шаг 9: Группировка данных (магия!)**
```python
# Средняя зарплата по возрастам
avg_salary_by_age = df.groupby('Возраст')['Зарплата'].mean()
print(avg_salary_by_age)

# Несколько агрегаций сразу
stats = df.groupby('Возраст').agg({
    'Зарплата': ['mean', 'min', 'max', 'count']
})
print(stats)
```

---

### **Шаг 10: Простые вычисления**
```python
# Сумма всех зарплат
total_salary = df['Зарплата'].sum()
print(f"Общая зарплата: {total_salary}")

# Средний возраст
average_age = df['Возраст'].mean()
print(f"Средний возраст: {average_age:.1f}")

# Максимальная зарплата
max_salary = df['Зарплата'].max()
print(f"Максимальная зарплата: {max_salary}")
```

---

### **Шаг 11: Сохранение результатов**
```python
# Сохраняем в CSV
df.to_csv('результаты.csv', index=False)

# Сохраняем в Excel
df.to_excel('результаты.xlsx', index=False)

# Без index=False в файле появится лишний столбец с номерами строк!
```

---

### **Шаг 12: Визуализация (простая)**
```python
import matplotlib.pyplot as plt

# Гистограмма возрастов
df['Возраст'].hist()
plt.title('Распределение возрастов')
plt.show()

# Столбчатая диаграмма зарплат
df.plot.bar(x='Имя', y='Зарплата')
plt.title('Зарплаты по сотрудникам')
plt.show()
```

**Важные ньюансы и команды:**

- **`df['столбец']`** — выбор одного столбца
- **`df[['столбец1', 'столбец2']]`** — выбор нескольких столбцов  
- **`df[df['условие']]`** — фильтрация строк
- **`df.iloc[индекс]`** — выбор по номеру строки
- **`axis=0`** — операции со строками
- **`axis=1`** — операции со столбцами

# NumPy

NumPy — это библиотека для работы с числами. Если Pandas — это Excel, то NumPy — это калькулятор на стероидах. Он умеет быстро делать математические операции с большими наборами чисел.

### **Шаг 1: Создание массивов (основа всего)**
```python
# Простой массив из списка
arr1 = np.array([1, 2, 3, 4, 5])
print(arr1)

# Двумерный массив (матрица)
arr2 = np.array([[1, 2, 3], [4, 5, 6]])
print(arr2)

# Трехмерный массив (куб)
arr3 = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])
print(arr3)
```

---

### **Шаг 2: Волшебные функции создания массивов**
```python
# Массив из нулей
zeros = np.zeros(5)  # [0, 0, 0, 0, 0]
print(zeros)

# Матрица из единиц
ones = np.ones((3, 3))  # 3x3 единицы
print(ones)

# Единичная матрица (главная диагональ = 1)
eye = np.eye(3)
print(eye)

# Диапазон чисел (как range, но лучше)
range_arr = np.arange(0, 10, 2)  # от 0 до 10 с шагом 2
print(range_arr)

# Равномерно распределенные числа
linspace = np.linspace(0, 1, 5)  # 5 чисел от 0 до 1
print(linspace)
```

---

### **Шаг 3: Случайные числа (самое веселое)**
```python
# Случайное число от 0 до 1
rand_num = np.random.random()
print(rand_num)

# Матрица случайных чисел
rand_matrix = np.random.random((2, 3))
print(rand_matrix)

# Случайные целые числа
rand_ints = np.random.randint(0, 100, 5)  # 5 чисел от 0 до 100
print(rand_ints)

# Нормальное распределение
normal = np.random.normal(0, 1, 5)  # среднее=0, std=1, 5 чисел
print(normal)
```

---

### **Шаг 4: Основные свойства массивов**
```python
arr = np.array([[1, 2, 3], [4, 5, 6]])

# Размер массива (сколько всего элементов)
print(arr.size)  # 6

# Форма массива (строки, столбцы)
print(arr.shape)  # (2, 3)

# Количество измерений
print(arr.ndim)  # 2

# Тип данных
print(arr.dtype)  # int64
```

---

### **Шаг 5: Изменение формы массивов**
```python
arr = np.arange(12)  # [0, 1, 2, ..., 11]
print(arr)

# Меняем форму на 3x4
reshaped = arr.reshape(3, 4)
print(reshaped)

# Делаем одномерным
flattened = reshaped.flatten()
print(flattened)

# Транспонирование (строки ↔ столбцы)
transposed = reshaped.T
print(transposed)
```

---

### **Шаг 6: Индексация и срезы (как в списках)**
```python
arr = np.array([10, 20, 30, 40, 50])

# Простая индексация
print(arr[0])    # 10
print(arr[-1])   # 50

# Срезы
print(arr[1:4])     # [20, 30, 40]
print(arr[::2])     # [10, 30, 50] (каждый второй)

# Многомерные массивы
arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

print(arr2d[1, 2])    # 6 (вторая строка, третий столбец)
print(arr2d[1, :])    # [4, 5, 6] (вся вторая строка)
print(arr2d[:, 1])    # [2, 5, 8] (второй столбец)
```

---

### **Шаг 7: Булева индексация (фильтрация)**
```python
arr = np.array([1, 2, 3, 4, 5, 6])

# Маска: где значения > 3?
mask = arr > 3
print(mask)  # [False, False, False, True, True, True]

# Применяем маску
filtered = arr[mask]
print(filtered)  # [4, 5, 6]

# В одну строку
big_numbers = arr[arr > 3]
print(big_numbers)  # [4, 5, 6]

# Несколько условий
middle = arr[(arr > 2) & (arr < 6)]  # И: &, ИЛИ: |, НЕ: ~
print(middle)  # [3, 4, 5]
```

---

### **Шаг 8: Математические операции**
```python
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

# Поэлементные операции
print(a + b)   # [5, 7, 9]
print(a * b)   # [4, 10, 18]
print(a ** 2)  # [1, 4, 9]

# Со скалярами
print(a + 10)   # [11, 12, 13]
print(a * 2)    # [2, 4, 6]

# Математические функции
print(np.sqrt(a))    # квадратный корень
print(np.exp(a))     # экспонента
print(np.sin(a))     # синус
print(np.log(a))     # натуральный логарифм
```

---

### **Шаг 9: Агрегирующие функции (статистика)**
```python
arr = np.array([1, 2, 3, 4, 5])

# Базовые статистики
print(np.sum(arr))      # 15
print(np.mean(arr))     # 3.0
print(np.min(arr))      # 1
print(np.max(arr))      # 5
print(np.std(arr))      # стандартное отклонение

# Для многомерных массивов
arr2d = np.array([[1, 2, 3], [4, 5, 6]])

# По столбцам (axis=0)
print(np.sum(arr2d, axis=0))  # [5, 7, 9]

# По строкам (axis=1)  
print(np.sum(arr2d, axis=1))  # [6, 15]
```

---

### **Шаг 10: Трансляция (Broadcasting)**
```python
# Матрица + вектор
matrix = np.array([[1, 2, 3], [4, 5, 6]])
vector = np.array([10, 20, 30])

# Вектор добавляется к каждой строке
result = matrix + vector
print(result)
# [[11, 22, 33],
#  [14, 25, 36]]

# Матрица + скаляр
result2 = matrix + 100
print(result2)
```

---

### **Шаг 11: Работа с файлами**
```python
# Сохраняем массив
arr = np.array([1, 2, 3, 4, 5])
np.save('my_array.npy', arr)

# Загружаем массив
loaded_arr = np.load('my_array.npy')
print(loaded_arr)

# Текстовые файлы
np.savetxt('array.txt', arr, delimiter=',')
loaded_txt = np.loadtxt('array.txt', delimiter=',')
print(loaded_txt)
```

---

### **Шаг 12: Практические примеры для химии**
```python
# Концентрации веществ в экспериментах
concentrations = np.array([0.1, 0.5, 1.0, 2.0, 5.0])

# Скорости реакции
rates = np.array([0.05, 0.25, 0.45, 0.85, 1.95])

# Линейная регрессия (подбор прямой)
k, b = np.polyfit(concentrations, rates, 1)
print(f"Константа скорости: {k:.3f}")
print(f"Сдвиг: {b:.3f}")

# Предсказание для новой концентрации
new_conc = 3.0
predicted_rate = k * new_conc + b
print(f"Предсказанная скорость: {predicted_rate:.3f}")
```

---

### **Шаг 13: Полезные трюки**
```python
# Генерация химических данных
molecular_weights = np.random.normal(200, 50, 100)  # 100 молекул
logP_values = np.random.normal(2, 1, 100)

# Поиск выбросов
outliers = molecular_weights[(molecular_weights > 300) | (molecular_weights < 100)]
print(f"Выбросы по массе: {outliers}")

# Нормализация данных
normalized = (molecular_weights - np.mean(molecular_weights)) / np.std(molecular_weights)
print(f"Нормализованные значения: {normalized[:5]}")

# Построение гистограммы (нужен matplotlib)
import matplotlib.pyplot as plt
plt.hist(molecular_weights, bins=20)
plt.title('Распределение молекулярных масс')
plt.show()
```


**Важные пометки:**

- **`np.array()`** — создание массива
- **`arr.shape`** — форма массива
- **`arr.reshape()`** — изменение формы
- **`arr[условие]`** — фильтрация
- **`axis=0`** — операции по столбцам
- **`axis=1`** — операции по строкам

# Sklearn

Scikit-learn (sklearn) — это главная библиотека для машинного обучения в Python. Инструмент для классификации, регрессии, кластеризации, снижения размерности, выбора моделей, а также предобработки данных.  

Шаг 1: Понимаем, что такое sklearn

    Классификация — определяем категории (спам/не спам)

    Регрессия — предсказываем числа (цену квартиры)

    Кластеризация — находим группы похожих данных

    Предобработка — готовим данные к работе 

Шаг 2: Базовый пайплайн (шаблон для ВСЕХ задач)

```python

# 1. Импортируем всё что нужно
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# 2. Загружаем данные
data = load_iris()
X = data.data      # Признаки (характеристики)
y = data.target    # Целевая переменная (что предсказываем)

# 3. Делим на тренировочную и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Масштабируем данные (очень важно!)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. Создаем и обучаем модель
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train_scaled, y_train)

# 6. Предсказываем и проверяем
predictions = model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, predictions)
print(f"Точность модели: {accuracy:.2f}")
```


Шаг 3: Самые полезные модели (копируй и используй)
```python

# Для классификации
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

# Для регрессии  
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor

# Для кластеризации
from sklearn.cluster import KMeans

# Просто подставляешь нужную модель в шаблон выше!
```

Шаг 4: Предобработка данных (магия превращения)
```python

# Стандартизация (делает данные с средним=0 и std=1)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Нормализация (приводит к диапазону 0-1)
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_normalized = scaler.fit_transform(X)

# Кодирование категориальных признаков
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder()
X_encoded = encoder.fit_transform(X)
```

Шаг 5: Оценка моделей (ставим оценки как строгий препод)
```python

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Для классификации
accuracy = accuracy_score(y_true, y_pred)
report = classification_report(y_true, y_pred)
matrix = confusion_matrix(y_true, y_pred)

# Для регрессии
from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_true, y_pred)
r2 = r2_score(y_true, y_pred)
```

Шаг 6: Подбор параметров (включаем турбо-режим)
```python

from sklearn.model_selection import GridSearchCV

# Задаем параметры для перебора
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Ищем лучшие параметры
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
grid_search.fit(X_train, y_train)

print(f"Лучшие параметры: {grid_search.best_params_}")
```

Шаг 7: Сохранение и загрузка моделей
```python

import joblib

# Сохраняем модель
joblib.dump(model, 'my_model.pkl')

# Загружаем модель  
loaded_model = joblib.load('my_model.pkl')

# Используем как обычно
predictions = loaded_model.predict(X_new)
```

Шаг 8: Практический пример — предсказание диабета
```python

from sklearn.datasets import load_diabetes
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Загружаем данные о диабете
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

# Делим и масштабируем
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Обучаем линейную регрессию
model = LinearRegression()
model.fit(X_train, y_train)

# Предсказываем и оцениваем
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
print(f"Качество предсказания: {r2:.3f}")
```

Что делать, если всё падает:

    Проверь версии: python -c "import sklearn; sklearn.show_versions()"

    Данные не масштабированы — используй StandardScaler

    Проблемы с установкой — попробуй: pip install --exists-action=i scikit-learn 

    Модель не обучается — проверь, что X и y правильных размеров

Памятка начинающего ~мамкиного~ ML-щика:

    Всегда масштабируй данные перед обучением

    Всегда дели данные на train/test

    Начинай с простых моделей (логистическая регрессия)

    Не бойся экспериментировать с параметрами

    Используй кросс-валидацию для надежных результатов

P.S. Если что-то не работает — sklearn обычно ругается на понятном(даже мне) английском. Читай сообщения об ошибках, они часто подсказывают решение.

# Matplotlib

Matplotlib — это главная библиотека для рисования графиков в Python.

Шаг 1: Основные компоненты (чтобы понимать, что происходит)

    Figure (Фигура) — это весь лист бумаги, на котором ты рисуешь

    Axes (Оси) — отдельный график на этом листе

    Axis (Ось) — конкретная ось X или Y 

```python

# Правильный способ создать фигуру и оси
fig, ax = plt.subplots()  # fig - бумага, ax - график
ax.plot(x, y)
plt.show()
```

Шаг 2: Оформление графиков (делаем красиво)
```python

plt.figure(figsize=(10, 6))  # Размер графика

plt.plot(x, y, 
         color='red',        # Цвет линии
         linestyle='--',     # Стиль линии (сплошная, пунктир и т.д.)
         marker='o',         # Маркеры в точках
         linewidth=2,        # Толщина линии
         label='Мои данные') # Подпись для легенды

plt.title('Мой первый красивый график', fontsize=14)
plt.xlabel('Ось X', fontsize=12)
plt.ylabel('Ось Y', fontsize=12)
plt.legend()  # Показываем легенду
plt.grid(True)  # Включаем сетку

plt.show()
```

Шаг 3: Основные типы графиков (копируй и используй)

Линейный график — для трендов и изменений во времени:
```python

plt.plot(x, y)
plt.title('Линейный график')
plt.show()

Столбчатая диаграмма — для сравнения категорий:
python

categories = ['A', 'B', 'C', 'D']
values = [25, 40, 30, 35]

plt.bar(categories, values, color=['red', 'blue', 'green', 'orange'])
plt.title('Столбчатая диаграмма')
plt.show()

Круговая диаграмма — для долей и процентов:
python

sizes = [30, 25, 15, 20, 10]
labels = ['Python', 'Java', 'C++', 'JavaScript', 'Другие']

plt.pie(sizes, labels=labels, autopct='%1.1f%%')
plt.title('Распределение языков программирования')
plt.show()
```

Гистограмма — для распределения данных:
```python

import numpy as np

data = np.random.normal(100, 15, 1000)  # Случайные данные
plt.hist(data, bins=30, alpha=0.7, color='skyblue')
plt.title('Гистограмма распределения')
plt.xlabel('Значения')
plt.ylabel('Частота')
plt.show()
```
Диаграмма рассеяния — для корреляции двух переменных:
```python

x = np.random.rand(50)
y = np.random.rand(50)

plt.scatter(x, y, alpha=0.6, color='green')
plt.title('Диаграмма рассеяния')
plt.xlabel('X')
plt.ylabel('Y')
plt.show()
```

Шаг 4: Несколько графиков на одном полотне
```python

# Создаем сетку графиков 2x2
fig, axes = plt.subplots(2, 2, figsize=(12, 8))

# График 1
axes[0, 0].plot(x, y, 'r-')
axes[0, 0].set_title('Линейный график')

# График 2  
axes[0, 1].bar(categories, values)
axes[0, 1].set_title('Столбчатая диаграмма')

# График 3
axes[1, 0].pie(sizes, labels=labels)
axes[1, 0].set_title('Круговая диаграмма')

# График 4
axes[1, 1].scatter(x, y)
axes[1, 1].set_title('Диаграмма рассеяния')

plt.tight_layout()  # Чтобы графики не наезжали друг на друга
plt.show()
```

Шаг 5: Работа с реальными данными (химический пример)
```python

# Данные по концентрациям и поглощению
concentrations = [0.1, 0.5, 1.0, 2.0, 5.0]
absorbance = [0.05, 0.25, 0.45, 0.85, 1.95]

plt.figure(figsize=(10, 6))
plt.scatter(concentrations, absorbance, s=100, alpha=0.7, 
           c=absorbance, cmap='viridis')

# Добавляем линию тренда
z = np.polyfit(concentrations, absorbance, 1)
p = np.poly1d(z)
plt.plot(concentrations, p(concentrations), "r--", 
         label=f'Линия тренда (R² = {z[0]:.3f}x + {z[1]:.3f})')

plt.colorbar(label='Поглощение')
plt.title('Калибровочная кривая', fontsize=14)
plt.xlabel('Концентрация, мг/мл', fontsize=12)
plt.ylabel('Оптическая плотность', fontsize=12)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

Шаг 6: Сохранение графиков
```python

# Сохраняем в разных форматах
plt.savefig('my_plot.png', dpi=300, bbox_inches='tight')  # PNG с высоким качеством
plt.savefig('my_plot.pdf')  # Векторный формат для публикаций
plt.savefig('my_plot.jpg', quality=90)  # JPEG
```

Шаг 7: Полезные трюки
```python

# Разные стили графиков
print(plt.style.available)  # Посмотреть доступные стили
plt.style.use('ggplot')  # Использовать стиль ggplot

# Математические формулы в тексте
plt.title(r'$\sigma = 15\ \mu=100$')  # Формулы в LaTeX

# Аннотации
plt.annotate('Пик значения', xy=(3, 17), xytext=(4, 18),
            arrowprops=dict(facecolor='black', shrink=0.05))

# Логарифмическая шкала
plt.yscale('log')
```

Что делать, если график не отображается:

    Не забывай plt.show() — особенно в скриптах

    В Jupyter используй %matplotlib inline — чтобы графики показывались в ноутбуке

    Проверь данные — нет ли пустых значений или нечисловых данных

    Убедись в импорте — import matplotlib.pyplot as plt
    
