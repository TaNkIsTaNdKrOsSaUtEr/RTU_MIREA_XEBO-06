**Машинное обучение для чайников от чайников (для химиков)**

**Что это за зверь и с чем его едят?**

Машинное обучение (ML) — это когда ты заставляешь компьютер угадывать ответы, как студент на экзамене, который прочитал только первую страницу учебника. Главное — накормить его правильными данными и надеяться, что он не переобуется на ходу.

##     **Содержание**
###      Основы
###      Pandas
###      Numpy
###      Sklearn
###      Matplotlib
###      Rdkit

# Основы

**Основные ингредиенты ML:**

1. **Данные** — твои реактивы. Без них нихуя не получится
2. **Признаки (т.н. features)** — характеристики молекул (масса, заряд, полярность и т.д.)
3. **Целевая переменная** — что мы хотим предсказать (активность, растворимость, токсичность)
4. **Модель** — собсна сама штука, которая обрабатывает все эти данные, признаки и т.п.

---

**Библиотеки — наши инстурменты**

```python
# Стандартный набор химика-MLщика
import pandas as pd  # Для табличек, типа Excel 
import numpy as np   # Для циферок и матриц
from sklearn import *  # Главная ML-библиотека
import matplotlib.pyplot as plt  # Чтобы рисовать красивые графики
from rdkit import Chem  # Химическая хуйня
```

**Установка всего этого добра:**
```bash
pip install pandas numpy scikit-learn matplotlib rdkit-pypi
```

---

**Подготовка данных (спиздить реактивы у соседей, занять место под вытяжкой и т.п.)**

**Загрузка данных:**
```python
# Вариант 1: из CSV файла
data = pd.read_csv('my_chemical_data.csv')

# Вариант 2: сгенерировать самому (когда лень искать реальные данные)
data = pd.DataFrame({
    'molecular_weight': np.random.normal(200, 50, 100),
    'logP': np.random.normal(2, 1, 100),
    'solubility': np.random.normal(-3, 1, 100)
})
```

**Предобработка (самая скучная часть):**
```python
# Удаляем строки с пропущенными значениями
data = data.dropna()

# Разделяем на признаки (X) и целевую переменную (y)
X = data[['molecular_weight', 'logP']]
y = data['solubility']

# Делим на обучающую и тестовую выборки
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

---

**Модели**

**Линейная регрессия** — как предсказать цену бухла по градусам:
```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)  # Обучаем модель
predictions = model.predict(X_test)  # Предсказываем
```

**Случайный лес** — когда не знаешь, какая модель лучше, но хочешь чтобы работало:
```python
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

**Метод k-ближайших соседей** — смотрим на похожих соседей:
```python
from sklearn.neighbors import KNeighborsRegressor

model = KNeighborsRegressor(n_neighbors=5)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

---

**RDKit — химическая хуйня**

**Чтение молекул из SMILES:**
```python
from rdkit import Chem
from rdkit.Chem import Draw

# Превращаем текстовые описания в молекулы
caffeine_smiles = 'CN1C=NC2=C1C(=O)N(C(=O)N2C)C'
caffeine = Chem.MolFromSmiles(caffeine_smiles)

# Рисуем молекулу (вау-эффект обеспечен)
img = Draw.MolToImage(caffeine)
img.save('caffeine.png')
```

**Вычисление молекулярных дескрипторов:**
```python
from rdkit.Chem import Descriptors

# Считаем молекулярную массу и логP
mw = Descriptors.MolWt(caffeine)
logp = Descriptors.MolLogP(caffeine)

print(f"Масса кофеина: {mw:.2f}")
print(f"LogP кофеина: {logp:.2f}")
```

**Молекулярные отпечатки:**
```python
from rdkit.Chem import AllChem

# Создаем отпечатки для ML
fp = AllChem.GetMorganFingerprintAsBitVect(caffeine, radius=2, nBits=1024)
fp_array = np.array(fp)  # Превращаем в массив для ML
```

---

**Оценка моделей — ставим оценки**

```python
from sklearn.metrics import mean_squared_error, r2_score

# Среднеквадратичная ошибка (чем меньше, тем лучше)
mse = mean_squared_error(y_test, predictions)

# R² score (чем ближе к 1, тем лучше)
r2 = r2_score(y_test, predictions)

print(f"MSE: {mse:.3f}")
print(f"R²: {r2:.3f}")

# Если R² отрицательный — твоя модель хуже, чем просто предсказывать среднее значение
# Поздравляю, ты достиг дна ML!
```

---

**Классификация — когда нужно просто "да/нет"**

**Логистическая регрессия** — для бинарной классификации (токсично/нет):
```python
from sklearn.linear_model import LogisticRegression

# Предположим, у нас есть бинарный таргет
y_binary = (y > y.median()).astype(int)  # Выше медиана = 1, ниже = 0

model = LogisticRegression()
model.fit(X_train, y_binary)
predictions = model.predict(X_test)

# Оценка точности
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, predictions)
print(f"Точность: {accuracy:.2f}")
```

---

**Перекрестная проверка — чтобы не обманывать себя**

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5, scoring='r2')
print(f"R² на кросс-валидации: {scores.mean():.3f} ± {scores.std():.3f}")
```

---

**Гиперпараметры — крутилки-настройки**

**GridSearchCV** — перебираем все возможные варианты:
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20]
}

grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5)
grid_search.fit(X_train, y_train)

print(f"Лучшие параметры: {grid_search.best_params_}")
```

---

**Кластеризация — ищем похожие молекулы**

**K-means** — группируем молекулы по похожести:
```python
from sklearn.cluster import KMeans

# Предположим, у нас есть матрица отпечатков пальцев
kmeans = KMeans(n_clusters=5, random_state=42)
clusters = kmeans.fit_predict(fingerprint_matrix)

print(f"Молекулы разбиты на {len(np.unique(clusters))} кластера")
```

---

**Нейронные сети — когда простые модели уже не кайф**

```python
from sklearn.neural_network import MLPRegressor

model = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000)
model.fit(X_train, y_train)

# Внимание: может потребоваться масштабирование данных!
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

---

**Типичные грабли, на которые ты наступишь**

1. **Утечка данных**
```python
# НЕПРАВИЛЬНО:
scaler.fit(X)  # Обучаем на всех данных
X_scaled = scaler.transform(X)

# ПРАВИЛЬНО:
scaler.fit(X_train)  # Обучаем только на тренировочных данных
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

2. **Переобучение** — когда модель запоминает ответы, но не понимает сути:
```python
# Симптомы: accuracy на train = 99%, на test = 50%
# Лечение: regularization, упрощение модели, больше данных
```

3. **Нормализация** — когда признаки в разных масштабах:
```python
# Молекулярная масса: 100-500
# LogP: -2 до 8
# Без нормализации модель будет обращать внимание только на массу!
```

---

**Полезные советы**

- **Начинайте с простых моделей** — линейная регрессия и случайный лес
- **Визуализируйте всё** — если не видно закономерность глазом, модель её тоже не найдет
- **Не верьте слепо метрикам** — смотрите на реальные предсказания
- **Вы умнее куска кода** — если модель предсказывает бред, скорее всего это бред

**Пример полного пайплайна:**
```python
# 1. Загрузка данных
data = pd.read_csv('chemical_data.csv')

# 2. Предобработка
X = data[['feature1', 'feature2', 'feature3']]
y = data['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 3. Масштабирование
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. Обучение модели
model = RandomForestRegressor(n_estimators=100)
model.fit(X_train_scaled, y_train)

# 5. Предсказание и оценка
predictions = model.predict(X_test_scaled)
print(f"R²: {r2_score(y_test, predictions):.3f}")
```

---

**Когда всё идет по пизде**

Если твоя модель работает хуже, чем предсказание среднего значения:

1. **Проверь данные** — нет ли пропусков, выбросов
2. **Убери коррелирующие признаки** 
3. **Попробуй другую модель**
4. **Собери больше данных**
5. **Пересмотри свой жизненный выбор, тебе точно нужна норм оценка по СИИБД?**

**Помни:** даже плохая ML-модель лучше, чем гадание на таро. Ну, почти всегда.

*P.S. Если что-то не работает — попробуй выключить и включить. Если не поможет — иди пить пиво.*

# Pandas

### **Шаг 1: Создание DataFrame (таблички)**
```python
# Способ 1: Из словаря (самый простой)
data = {
    'Имя': ['Вася', 'Петя', 'Маша', 'Даша'],
    'Возраст': [25, 30, 35, 28],
    'Зарплата': [50000, 60000, 70000, 55000]
}

df = pd.DataFrame(data)
print(df)
```
```
   Имя  Возраст  Зарплата
0  Вася      25     50000
1  Петя      30     60000  
2  Маша      35     70000
3  Даша      28     55000
```

---

### **Шаг 2: Чтение данных из файла**
```python
# Читаем CSV файл Если файл в той же папке, что и твой код
df = pd.read_csv('данные_химия.csv')

# Посмотреть первые 5 строк
print(df.head())
```

---

### **Шаг 3: Основные команды для просмотра данных**
```python
# Первые 3 строки
df.head(3)

# Последние 3 строки  
df.tail(3)

# Размер таблицы (строки, столбцы)
print(df.shape)

# Общая информация о данных
df.info()

# Статистика по числовым столбцам
df.describe()
```

---

### **Шаг 4: Выбор данных (самое важное!)**
```python
# Выбор одного столбца
ages = df['Возраст']
print(ages)

# Выбор нескольких столбцов
subset = df[['Имя', 'Зарплата']]
print(subset)

# Выбор строк по индексу
row_2 = df.iloc[2]  # Третья строка (индексы с 0)
print(row_2)

# Выбор диапазона строк
first_3 = df.iloc[0:3]  # Строки 0, 1, 2
print(first_3)
```

---

### **Шаг 5: Фильтрация данных**
```python
# Все, кто старше 28 лет
old_people = df[df['Возраст'] > 28]
print(old_people)

# Люди с зарплатой от 50к до 65к
middle_salary = df[(df['Зарплата'] >= 50000) & (df['Зарплата'] <= 65000)]
print(middle_salary)

# Фильтр по тексту (все Васи)
vasyas = df[df['Имя'] == 'Вася']
print(vasyas)
```

---

### **Шаг 6: Добавление и удаление столбцов**
```python
# Добавляем новый столбец
df['Премия'] = df['Зарплата'] * 0.1  # 10% от зарплаты
print(df)

# Удаляем столбец
df = df.drop('Премия', axis=1)  # axis=1 значит "столбец"
print(df)

# Можно удалить несколько
df = df.drop(['Столбец1', 'Столбец2'], axis=1)
```

---

### **Шаг 7: Работа с пропущенными значениями**
```python
# Проверяем есть ли пропуски
print(df.isnull().sum())

# Заполняем пропуски нулями
df_filled = df.fillna(0)

# Удаляем строки с пропусками
df_clean = df.dropna()
```

---

### **Шаг 8: Сортировка**
```python
# Сортировка по возрасту (по возрастанию)
df_sorted = df.sort_values('Возраст')
print(df_sorted)

# Сортировка по зарплате (по убыванию)
df_sorted_desc = df.sort_values('Зарплата', ascending=False)
print(df_sorted_desc)
```

---

### **Шаг 9: Группировка данных (магия!)**
```python
# Средняя зарплата по возрастам
avg_salary_by_age = df.groupby('Возраст')['Зарплата'].mean()
print(avg_salary_by_age)

# Несколько агрегаций сразу
stats = df.groupby('Возраст').agg({
    'Зарплата': ['mean', 'min', 'max', 'count']
})
print(stats)
```

---

### **Шаг 10: Простые вычисления**
```python
# Сумма всех зарплат
total_salary = df['Зарплата'].sum()
print(f"Общая зарплата: {total_salary}")

# Средний возраст
average_age = df['Возраст'].mean()
print(f"Средний возраст: {average_age:.1f}")

# Максимальная зарплата
max_salary = df['Зарплата'].max()
print(f"Максимальная зарплата: {max_salary}")
```

---

### **Шаг 11: Сохранение результатов**
```python
# Сохраняем в CSV
df.to_csv('результаты.csv', index=False)

# Сохраняем в Excel
df.to_excel('результаты.xlsx', index=False)

# Без index=False в файле появится лишний столбец с номерами строк!
```

---

### **Шаг 12: Визуализация (простая)**
```python
import matplotlib.pyplot as plt

# Гистограмма возрастов
df['Возраст'].hist()
plt.title('Распределение возрастов')
plt.show()

# Столбчатая диаграмма зарплат
df.plot.bar(x='Имя', y='Зарплата')
plt.title('Зарплаты по сотрудникам')
plt.show()
```

**Важные ньюансы и команды:**

- **`df['столбец']`** — выбор одного столбца
- **`df[['столбец1', 'столбец2']]`** — выбор нескольких столбцов  
- **`df[df['условие']]`** — фильтрация строк
- **`df.iloc[индекс]`** — выбор по номеру строки
- **`axis=0`** — операции со строками
- **`axis=1`** — операции со столбцами



