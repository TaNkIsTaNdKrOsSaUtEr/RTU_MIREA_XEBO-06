**Машинное обучение для чайников от чайников (для химиков)**

**Что это за зверь и с чем его едят?**

Машинное обучение (ML) — это когда ты заставляешь компьютер угадывать ответы, дав косвенно связанные исходные данные. Главное — накормить его правильными данными и надеяться, что он не проебет их.

##     **Содержание**
###      Основы
###      Pandas
###      Numpy
###      Sklearn
###      Matplotlib
###      Rdkit
###      Словарь

# Основы

**Основные ингредиенты ML:**

1. **Данные** — твои реактивы. Без них нихуя не получится
2. **Признаки (т.н. features)** — характеристики молекул (масса, заряд, полярность и т.д.)
3. **Целевая переменная** — что мы хотим предсказать (активность, растворимость, токсичность)
4. **Модель** — собсна сама штука, которая обрабатывает все эти данные, признаки и т.п.

---

**Библиотеки — наши инстурменты**

```python
# Стандартный набор химика-MLщика
import pandas as pd  # Для табличек, типа Excel 
import numpy as np   # Для циферок и матриц
from sklearn import *  # Главная ML-библиотека
import matplotlib.pyplot as plt  # Чтобы рисовать красивые графики
from rdkit import Chem  # Химическая хуйня
```

**Установка всего этого добра:**
```bash
pip install pandas numpy scikit-learn matplotlib rdkit-pypi
```

---

**Подготовка данных (спиздить реактивы у соседей, занять место под вытяжкой и т.п.)**

**Загрузка данных:**
```python
# Вариант 1: из CSV файла
data = pd.read_csv('my_chemical_data.csv')

# Вариант 2: сгенерировать самому (когда лень искать реальные данные)
data = pd.DataFrame({
    'molecular_weight': np.random.normal(200, 50, 100),
    'logP': np.random.normal(2, 1, 100),
    'solubility': np.random.normal(-3, 1, 100)
})
```

**Предобработка (самая скучная часть):**
```python
# Удаляем строки с пропущенными значениями
data = data.dropna()

# Разделяем на признаки (X) и целевую переменную (y)
X = data[['molecular_weight', 'logP']]
y = data['solubility']

# Делим на обучающую и тестовую выборки
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

---

**Модели**

**Линейная регрессия** — как предсказать цену бухла по градусам:
```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)  # Обучаем модель
predictions = model.predict(X_test)  # Предсказываем
```

**Случайный лес** — когда не знаешь, какая модель лучше, но хочешь чтобы работало:
```python
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

**Метод k-ближайших соседей** — смотрим на похожих соседей:
```python
from sklearn.neighbors import KNeighborsRegressor

model = KNeighborsRegressor(n_neighbors=5)
model.fit(X_train, y_train)
predictions = model.predict(X_test)
```

---

**RDKit — химическая хуйня**

**Чтение молекул из SMILES:**
```python
from rdkit import Chem
from rdkit.Chem import Draw

# Превращаем текстовые описания в молекулы
caffeine_smiles = 'CN1C=NC2=C1C(=O)N(C(=O)N2C)C'
caffeine = Chem.MolFromSmiles(caffeine_smiles)

# Рисуем молекулу (вау-эффект обеспечен)
img = Draw.MolToImage(caffeine)
img.save('caffeine.png')
```

**Вычисление молекулярных дескрипторов:**
```python
from rdkit.Chem import Descriptors

# Считаем молекулярную массу и логP
mw = Descriptors.MolWt(caffeine)
logp = Descriptors.MolLogP(caffeine)

print(f"Масса кофеина: {mw:.2f}")
print(f"LogP кофеина: {logp:.2f}")
```

**Молекулярные отпечатки:**
```python
from rdkit.Chem import AllChem

# Создаем отпечатки для ML
fp = AllChem.GetMorganFingerprintAsBitVect(caffeine, radius=2, nBits=1024)
fp_array = np.array(fp)  # Превращаем в массив для ML
```

---

**Оценка моделей — ставим оценки**

```python
from sklearn.metrics import mean_squared_error, r2_score

# Среднеквадратичная ошибка (чем меньше, тем лучше)
mse = mean_squared_error(y_test, predictions)

# R² score (чем ближе к 1, тем лучше)
r2 = r2_score(y_test, predictions)

print(f"MSE: {mse:.3f}")
print(f"R²: {r2:.3f}")

# Если R² отрицательный — твоя модель хуже, чем просто предсказывать среднее значение
# Поздравляю, ты достиг дна ML!
```

---

**Классификация — когда нужно просто "да/нет"**

**Логистическая регрессия** — для бинарной классификации (токсично/нет):
```python
from sklearn.linear_model import LogisticRegression

# Предположим, у нас есть бинарный таргет
y_binary = (y > y.median()).astype(int)  # Выше медиана = 1, ниже = 0

model = LogisticRegression()
model.fit(X_train, y_binary)
predictions = model.predict(X_test)

# Оценка точности
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, predictions)
print(f"Точность: {accuracy:.2f}")
```

---

**Перекрестная проверка — чтобы не обманывать себя**

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5, scoring='r2')
print(f"R² на кросс-валидации: {scores.mean():.3f} ± {scores.std():.3f}")
```

---

**Гиперпараметры — крутилки-настройки**

**GridSearchCV** — перебираем все возможные варианты:
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20]
}

grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5)
grid_search.fit(X_train, y_train)

print(f"Лучшие параметры: {grid_search.best_params_}")
```

---

**Кластеризация — ищем похожие молекулы**

**K-means** — группируем молекулы по похожести:
```python
from sklearn.cluster import KMeans

# Предположим, у нас есть матрица отпечатков пальцев
kmeans = KMeans(n_clusters=5, random_state=42)
clusters = kmeans.fit_predict(fingerprint_matrix)

print(f"Молекулы разбиты на {len(np.unique(clusters))} кластера")
```

---

**Нейронные сети — когда простые модели уже не кайфы или тупо не тянут**

```python
from sklearn.neural_network import MLPRegressor

model = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000)
model.fit(X_train, y_train)

# Внимание: может потребоваться масштабирование данных!
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

---

**Типичные грабли, на которые ты наступишь**

1. **Утечка данных**
```python
# НЕПРАВИЛЬНО:
scaler.fit(X)  # Обучаем на всех данных
X_scaled = scaler.transform(X)

# ПРАВИЛЬНО:
scaler.fit(X_train)  # Обучаем только на тренировочных данных
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

2. **Переобучение** — когда модель запоминает ответы, но не понимает сути:
```python
# Симптомы: accuracy на train = 99%, на test = 50%
# Лечение: regularization, упрощение модели, больше данных
```

3. **Нормализация** — когда признаки в разных масштабах:
```python
# Молекулярная масса: 100-500
# LogP: -2 до 8
# Без нормализации модель будет обращать внимание только на массу!
```

---

**Полезные советы**

- **Начинайте с простых моделей** — линейная регрессия и случайный лес
- **Визуализируйте всё** — если не видно закономерность глазом, модель её тоже не найдет
- **Не верьте слепо метрикам** — смотрите на реальные предсказания
- **Вы умнее куска кода** — если модель предсказывает бред, скорее всего это бред

**Пример полного пайплайна:**
```python
# 1. Загрузка данных
data = pd.read_csv('chemical_data.csv')

# 2. Предобработка
X = data[['feature1', 'feature2', 'feature3']]
y = data['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 3. Масштабирование
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 4. Обучение модели
model = RandomForestRegressor(n_estimators=100)
model.fit(X_train_scaled, y_train)

# 5. Предсказание и оценка
predictions = model.predict(X_test_scaled)
print(f"R²: {r2_score(y_test, predictions):.3f}")
```

---

**Когда всё идет по пизде**

Если твоя модель работает хуже, чем предсказание среднего значения:

1. **Проверь данные** — нет ли пропусков, выбросов
2. **Убери коррелирующие признаки** 
3. **Попробуй другую модель**
4. **Собери больше данных**
5. **Пересмотри свой жизненный выбор, тебе точно нужна норм оценка по СИИБД?**

**Помни:** даже плохая ML-модель лучше, чем гадание на таро. Ну, почти всегда.

*P.S. Если что-то не работает — попробуй выключить и включить. Если не поможет — иди пить пиво.*

# Pandas

Pandas - Excel по питоновски

### **Шаг 1: Создание DataFrame (таблички)**
```python
# Способ 1: Из словаря (самый простой)
data = {
    'Имя': ['Вася', 'Петя', 'Маша', 'Даша'],
    'Возраст': [25, 30, 35, 28],
    'Зарплата': [50000, 60000, 70000, 55000]
}

df = pd.DataFrame(data)
print(df)
```
```
   Имя  Возраст  Зарплата
0  Вася      25     50000
1  Петя      30     60000  
2  Маша      35     70000
3  Даша      28     55000
```

---

### **Шаг 2: Чтение данных из файла**
```python
# Читаем CSV файл Если файл в той же папке, что и твой код
df = pd.read_csv('данные_химия.csv')

# Посмотреть первые 5 строк
print(df.head())
```

---

### **Шаг 3: Основные команды для просмотра данных**
```python
# Первые 3 строки
df.head(3)

# Последние 3 строки  
df.tail(3)

# Размер таблицы (строки, столбцы)
print(df.shape)

# Общая информация о данных
df.info()

# Статистика по числовым столбцам
df.describe()
```

---

### **Шаг 4: Выбор данных (самое важное!)**
```python
# Выбор одного столбца
ages = df['Возраст']
print(ages)

# Выбор нескольких столбцов
subset = df[['Имя', 'Зарплата']]
print(subset)

# Выбор строк по индексу
row_2 = df.iloc[2]  # Третья строка (индексы с 0)
print(row_2)

# Выбор диапазона строк
first_3 = df.iloc[0:3]  # Строки 0, 1, 2
print(first_3)
```

---

### **Шаг 5: Фильтрация данных**
```python
# Все, кто старше 28 лет
old_people = df[df['Возраст'] > 28]
print(old_people)

# Люди с зарплатой от 50к до 65к
middle_salary = df[(df['Зарплата'] >= 50000) & (df['Зарплата'] <= 65000)]
print(middle_salary)

# Фильтр по тексту (все Васи)
vasyas = df[df['Имя'] == 'Вася']
print(vasyas)
```

---

### **Шаг 6: Добавление и удаление столбцов**
```python
# Добавляем новый столбец
df['Премия'] = df['Зарплата'] * 0.1  # 10% от зарплаты
print(df)

# Удаляем столбец
df = df.drop('Премия', axis=1)  # axis=1 значит "столбец"
print(df)

# Можно удалить несколько
df = df.drop(['Столбец1', 'Столбец2'], axis=1)
```

---

### **Шаг 7: Работа с пропущенными значениями**
```python
# Проверяем есть ли пропуски
print(df.isnull().sum())

# Заполняем пропуски нулями
df_filled = df.fillna(0)

# Удаляем строки с пропусками
df_clean = df.dropna()
```

---

### **Шаг 8: Сортировка**
```python
# Сортировка по возрасту (по возрастанию)
df_sorted = df.sort_values('Возраст')
print(df_sorted)

# Сортировка по зарплате (по убыванию)
df_sorted_desc = df.sort_values('Зарплата', ascending=False)
print(df_sorted_desc)
```

---

### **Шаг 9: Группировка данных (магия!)**
```python
# Средняя зарплата по возрастам
avg_salary_by_age = df.groupby('Возраст')['Зарплата'].mean()
print(avg_salary_by_age)

# Несколько агрегаций сразу
stats = df.groupby('Возраст').agg({
    'Зарплата': ['mean', 'min', 'max', 'count']
})
print(stats)
```

---

### **Шаг 10: Простые вычисления**
```python
# Сумма всех зарплат
total_salary = df['Зарплата'].sum()
print(f"Общая зарплата: {total_salary}")

# Средний возраст
average_age = df['Возраст'].mean()
print(f"Средний возраст: {average_age:.1f}")

# Максимальная зарплата
max_salary = df['Зарплата'].max()
print(f"Максимальная зарплата: {max_salary}")
```

---

### **Шаг 11: Сохранение результатов**
```python
# Сохраняем в CSV
df.to_csv('результаты.csv', index=False)

# Сохраняем в Excel
df.to_excel('результаты.xlsx', index=False)

# Без index=False в файле появится лишний столбец с номерами строк!
```

---

### **Шаг 12: Визуализация (простая)**
```python
import matplotlib.pyplot as plt

# Гистограмма возрастов
df['Возраст'].hist()
plt.title('Распределение возрастов')
plt.show()

# Столбчатая диаграмма зарплат
df.plot.bar(x='Имя', y='Зарплата')
plt.title('Зарплаты по сотрудникам')
plt.show()
```

**Важные ньюансы и команды:**

- **`df['столбец']`** — выбор одного столбца
- **`df[['столбец1', 'столбец2']]`** — выбор нескольких столбцов  
- **`df[df['условие']]`** — фильтрация строк
- **`df.iloc[индекс]`** — выбор по номеру строки
- **`axis=0`** — операции со строками
- **`axis=1`** — операции со столбцами

# NumPy

NumPy — это библиотека для работы с числами. Если Pandas — это Excel, то NumPy — это калькулятор на стероидах. Он умеет быстро делать математические операции с большими наборами чисел.

### **Шаг 1: Создание массивов (основа всего)**
```python
# Простой массив из списка
arr1 = np.array([1, 2, 3, 4, 5])
print(arr1)

# Двумерный массив (матрица)
arr2 = np.array([[1, 2, 3], [4, 5, 6]])
print(arr2)

# Трехмерный массив (куб)
arr3 = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])
print(arr3)
```

---

### **Шаг 2: Волшебные функции создания массивов**
```python
# Массив из нулей
zeros = np.zeros(5)  # [0, 0, 0, 0, 0]
print(zeros)

# Матрица из единиц
ones = np.ones((3, 3))  # 3x3 единицы
print(ones)

# Единичная матрица (главная диагональ = 1)
eye = np.eye(3)
print(eye)

# Диапазон чисел (как range, но лучше)
range_arr = np.arange(0, 10, 2)  # от 0 до 10 с шагом 2
print(range_arr)

# Равномерно распределенные числа
linspace = np.linspace(0, 1, 5)  # 5 чисел от 0 до 1
print(linspace)
```

---

### **Шаг 3: Случайные числа (самое веселое)**
```python
# Случайное число от 0 до 1
rand_num = np.random.random()
print(rand_num)

# Матрица случайных чисел
rand_matrix = np.random.random((2, 3))
print(rand_matrix)

# Случайные целые числа
rand_ints = np.random.randint(0, 100, 5)  # 5 чисел от 0 до 100
print(rand_ints)

# Нормальное распределение
normal = np.random.normal(0, 1, 5)  # среднее=0, std=1, 5 чисел
print(normal)
```

---

### **Шаг 4: Основные свойства массивов**
```python
arr = np.array([[1, 2, 3], [4, 5, 6]])

# Размер массива (сколько всего элементов)
print(arr.size)  # 6

# Форма массива (строки, столбцы)
print(arr.shape)  # (2, 3)

# Количество измерений
print(arr.ndim)  # 2

# Тип данных
print(arr.dtype)  # int64
```

---

### **Шаг 5: Изменение формы массивов**
```python
arr = np.arange(12)  # [0, 1, 2, ..., 11]
print(arr)

# Меняем форму на 3x4
reshaped = arr.reshape(3, 4)
print(reshaped)

# Делаем одномерным
flattened = reshaped.flatten()
print(flattened)

# Транспонирование (строки ↔ столбцы)
transposed = reshaped.T
print(transposed)
```

---

### **Шаг 6: Индексация и срезы (как в списках)**
```python
arr = np.array([10, 20, 30, 40, 50])

# Простая индексация
print(arr[0])    # 10
print(arr[-1])   # 50

# Срезы
print(arr[1:4])     # [20, 30, 40]
print(arr[::2])     # [10, 30, 50] (каждый второй)

# Многомерные массивы
arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

print(arr2d[1, 2])    # 6 (вторая строка, третий столбец)
print(arr2d[1, :])    # [4, 5, 6] (вся вторая строка)
print(arr2d[:, 1])    # [2, 5, 8] (второй столбец)
```

---

### **Шаг 7: Булева индексация (фильтрация)**
```python
arr = np.array([1, 2, 3, 4, 5, 6])

# Маска: где значения > 3?
mask = arr > 3
print(mask)  # [False, False, False, True, True, True]

# Применяем маску
filtered = arr[mask]
print(filtered)  # [4, 5, 6]

# В одну строку
big_numbers = arr[arr > 3]
print(big_numbers)  # [4, 5, 6]

# Несколько условий
middle = arr[(arr > 2) & (arr < 6)]  # И: &, ИЛИ: |, НЕ: ~
print(middle)  # [3, 4, 5]
```

---

### **Шаг 8: Математические операции**
```python
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

# Поэлементные операции
print(a + b)   # [5, 7, 9]
print(a * b)   # [4, 10, 18]
print(a ** 2)  # [1, 4, 9]

# Со скалярами
print(a + 10)   # [11, 12, 13]
print(a * 2)    # [2, 4, 6]

# Математические функции
print(np.sqrt(a))    # квадратный корень
print(np.exp(a))     # экспонента
print(np.sin(a))     # синус
print(np.log(a))     # натуральный логарифм
```

---

### **Шаг 9: Агрегирующие функции (статистика)**
```python
arr = np.array([1, 2, 3, 4, 5])

# Базовые статистики
print(np.sum(arr))      # 15
print(np.mean(arr))     # 3.0
print(np.min(arr))      # 1
print(np.max(arr))      # 5
print(np.std(arr))      # стандартное отклонение

# Для многомерных массивов
arr2d = np.array([[1, 2, 3], [4, 5, 6]])

# По столбцам (axis=0)
print(np.sum(arr2d, axis=0))  # [5, 7, 9]

# По строкам (axis=1)  
print(np.sum(arr2d, axis=1))  # [6, 15]
```

---

### **Шаг 10: Трансляция (Broadcasting)**
```python
# Матрица + вектор
matrix = np.array([[1, 2, 3], [4, 5, 6]])
vector = np.array([10, 20, 30])

# Вектор добавляется к каждой строке
result = matrix + vector
print(result)
# [[11, 22, 33],
#  [14, 25, 36]]

# Матрица + скаляр
result2 = matrix + 100
print(result2)
```

---

### **Шаг 11: Работа с файлами**
```python
# Сохраняем массив
arr = np.array([1, 2, 3, 4, 5])
np.save('my_array.npy', arr)

# Загружаем массив
loaded_arr = np.load('my_array.npy')
print(loaded_arr)

# Текстовые файлы
np.savetxt('array.txt', arr, delimiter=',')
loaded_txt = np.loadtxt('array.txt', delimiter=',')
print(loaded_txt)
```

---

### **Шаг 12: Практические примеры для химии**
```python
# Концентрации веществ в экспериментах
concentrations = np.array([0.1, 0.5, 1.0, 2.0, 5.0])

# Скорости реакции
rates = np.array([0.05, 0.25, 0.45, 0.85, 1.95])

# Линейная регрессия (подбор прямой)
k, b = np.polyfit(concentrations, rates, 1)
print(f"Константа скорости: {k:.3f}")
print(f"Сдвиг: {b:.3f}")

# Предсказание для новой концентрации
new_conc = 3.0
predicted_rate = k * new_conc + b
print(f"Предсказанная скорость: {predicted_rate:.3f}")
```

---

### **Шаг 13: Полезные трюки**
```python
# Генерация химических данных
molecular_weights = np.random.normal(200, 50, 100)  # 100 молекул
logP_values = np.random.normal(2, 1, 100)

# Поиск выбросов
outliers = molecular_weights[(molecular_weights > 300) | (molecular_weights < 100)]
print(f"Выбросы по массе: {outliers}")

# Нормализация данных
normalized = (molecular_weights - np.mean(molecular_weights)) / np.std(molecular_weights)
print(f"Нормализованные значения: {normalized[:5]}")

# Построение гистограммы (нужен matplotlib)
import matplotlib.pyplot as plt
plt.hist(molecular_weights, bins=20)
plt.title('Распределение молекулярных масс')
plt.show()
```


**Важные пометки:**

- **`np.array()`** — создание массива
- **`arr.shape`** — форма массива
- **`arr.reshape()`** — изменение формы
- **`arr[условие]`** — фильтрация
- **`axis=0`** — операции по столбцам
- **`axis=1`** — операции по строкам

# Sklearn

Scikit-learn (sklearn) — это главная библиотека для машинного обучения в Python. Инструмент для классификации, регрессии, кластеризации, снижения размерности, выбора моделей, а также предобработки данных.  

Шаг 1: Понимаем, что такое sklearn

    Классификация — определяем категории (спам/не спам)

    Регрессия — предсказываем числа (цену квартиры)

    Кластеризация — находим группы похожих данных

    Предобработка — готовим данные к работе 

Шаг 2: Базовый пайплайн (шаблон для ВСЕХ задач)

```python

# 1. Импортируем всё что нужно
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# 2. Загружаем данные
data = load_iris()
X = data.data      # Признаки (характеристики)
y = data.target    # Целевая переменная (что предсказываем)

# 3. Делим на тренировочную и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Масштабируем данные (очень важно!)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. Создаем и обучаем модель
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train_scaled, y_train)

# 6. Предсказываем и проверяем
predictions = model.predict(X_test_scaled)
accuracy = accuracy_score(y_test, predictions)
print(f"Точность модели: {accuracy:.2f}")
```


Шаг 3: Самые полезные модели (копируй и используй)
```python

# Для классификации
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

# Для регрессии  
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor

# Для кластеризации
from sklearn.cluster import KMeans

# Просто подставляешь нужную модель в шаблон выше!
```

Шаг 4: Предобработка данных (магия превращения)
```python

# Стандартизация (делает данные с средним=0 и std=1)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Нормализация (приводит к диапазону 0-1)
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_normalized = scaler.fit_transform(X)

# Кодирование категориальных признаков
from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder()
X_encoded = encoder.fit_transform(X)
```

Шаг 5: Оценка моделей (ставим оценки как строгий препод)
```python

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Для классификации
accuracy = accuracy_score(y_true, y_pred)
report = classification_report(y_true, y_pred)
matrix = confusion_matrix(y_true, y_pred)

# Для регрессии
from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_true, y_pred)
r2 = r2_score(y_true, y_pred)
```

Шаг 6: Подбор параметров (включаем турбо-режим)
```python

from sklearn.model_selection import GridSearchCV

# Задаем параметры для перебора
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10]
}

# Ищем лучшие параметры
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
grid_search.fit(X_train, y_train)

print(f"Лучшие параметры: {grid_search.best_params_}")
```

Шаг 7: Сохранение и загрузка моделей
```python

import joblib

# Сохраняем модель
joblib.dump(model, 'my_model.pkl')

# Загружаем модель  
loaded_model = joblib.load('my_model.pkl')

# Используем как обычно
predictions = loaded_model.predict(X_new)
```

Шаг 8: Практический пример — предсказание диабета
```python

from sklearn.datasets import load_diabetes
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

# Загружаем данные о диабете
diabetes = load_diabetes()
X, y = diabetes.data, diabetes.target

# Делим и масштабируем
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Обучаем линейную регрессию
model = LinearRegression()
model.fit(X_train, y_train)

# Предсказываем и оцениваем
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
print(f"Качество предсказания: {r2:.3f}")
```

Что делать, если всё падает:

    Проверь версии: python -c "import sklearn; sklearn.show_versions()"

    Данные не масштабированы — используй StandardScaler

    Проблемы с установкой — попробуй: pip install --exists-action=i scikit-learn 

    Модель не обучается — проверь, что X и y правильных размеров

Памятка начинающего ~мамкиного~ ML-щика:

    Всегда масштабируй данные перед обучением

    Всегда дели данные на train/test

    Начинай с простых моделей (логистическая регрессия)

    Не бойся экспериментировать с параметрами

    Используй кросс-валидацию для надежных результатов

P.S. Если что-то не работает — sklearn обычно ругается на понятном(даже мне) английском. Читай сообщения об ошибках, они часто подсказывают решение.

# Matplotlib

Matplotlib — это главная библиотека для рисования графиков в Python.

Шаг 1: Основные компоненты (чтобы понимать, что происходит)

    Figure (Фигура) — это весь лист бумаги, на котором ты рисуешь

    Axes (Оси) — отдельный график на этом листе

    Axis (Ось) — конкретная ось X или Y 

```python

# Правильный способ создать фигуру и оси
fig, ax = plt.subplots()  # fig - бумага, ax - график
ax.plot(x, y)
plt.show()
```

Шаг 2: Оформление графиков (делаем красиво)
```python

plt.figure(figsize=(10, 6))  # Размер графика

plt.plot(x, y, 
         color='red',        # Цвет линии
         linestyle='--',     # Стиль линии (сплошная, пунктир и т.д.)
         marker='o',         # Маркеры в точках
         linewidth=2,        # Толщина линии
         label='Мои данные') # Подпись для легенды

plt.title('Мой первый красивый график', fontsize=14)
plt.xlabel('Ось X', fontsize=12)
plt.ylabel('Ось Y', fontsize=12)
plt.legend()  # Показываем легенду
plt.grid(True)  # Включаем сетку

plt.show()
```

Шаг 3: Основные типы графиков (копируй и используй)

Линейный график — для трендов и изменений во времени:
```python

plt.plot(x, y)
plt.title('Линейный график')
plt.show()

Столбчатая диаграмма — для сравнения категорий:
python

categories = ['A', 'B', 'C', 'D']
values = [25, 40, 30, 35]

plt.bar(categories, values, color=['red', 'blue', 'green', 'orange'])
plt.title('Столбчатая диаграмма')
plt.show()

Круговая диаграмма — для долей и процентов:
python

sizes = [30, 25, 15, 20, 10]
labels = ['Python', 'Java', 'C++', 'JavaScript', 'Другие']

plt.pie(sizes, labels=labels, autopct='%1.1f%%')
plt.title('Распределение языков программирования')
plt.show()
```

Гистограмма — для распределения данных:
```python

import numpy as np

data = np.random.normal(100, 15, 1000)  # Случайные данные
plt.hist(data, bins=30, alpha=0.7, color='skyblue')
plt.title('Гистограмма распределения')
plt.xlabel('Значения')
plt.ylabel('Частота')
plt.show()
```
Диаграмма рассеяния — для корреляции двух переменных:
```python

x = np.random.rand(50)
y = np.random.rand(50)

plt.scatter(x, y, alpha=0.6, color='green')
plt.title('Диаграмма рассеяния')
plt.xlabel('X')
plt.ylabel('Y')
plt.show()
```

Шаг 4: Несколько графиков на одном полотне
```python

# Создаем сетку графиков 2x2
fig, axes = plt.subplots(2, 2, figsize=(12, 8))

# График 1
axes[0, 0].plot(x, y, 'r-')
axes[0, 0].set_title('Линейный график')

# График 2  
axes[0, 1].bar(categories, values)
axes[0, 1].set_title('Столбчатая диаграмма')

# График 3
axes[1, 0].pie(sizes, labels=labels)
axes[1, 0].set_title('Круговая диаграмма')

# График 4
axes[1, 1].scatter(x, y)
axes[1, 1].set_title('Диаграмма рассеяния')

plt.tight_layout()  # Чтобы графики не наезжали друг на друга
plt.show()
```

Шаг 5: Работа с реальными данными (химический пример)
```python

# Данные по концентрациям и поглощению
concentrations = [0.1, 0.5, 1.0, 2.0, 5.0]
absorbance = [0.05, 0.25, 0.45, 0.85, 1.95]

plt.figure(figsize=(10, 6))
plt.scatter(concentrations, absorbance, s=100, alpha=0.7, 
           c=absorbance, cmap='viridis')

# Добавляем линию тренда
z = np.polyfit(concentrations, absorbance, 1)
p = np.poly1d(z)
plt.plot(concentrations, p(concentrations), "r--", 
         label=f'Линия тренда (R² = {z[0]:.3f}x + {z[1]:.3f})')

plt.colorbar(label='Поглощение')
plt.title('Калибровочная кривая', fontsize=14)
plt.xlabel('Концентрация, мг/мл', fontsize=12)
plt.ylabel('Оптическая плотность', fontsize=12)
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

Шаг 6: Сохранение графиков
```python

# Сохраняем в разных форматах
plt.savefig('my_plot.png', dpi=300, bbox_inches='tight')  # PNG с высоким качеством
plt.savefig('my_plot.pdf')  # Векторный формат для публикаций
plt.savefig('my_plot.jpg', quality=90)  # JPEG
```

Шаг 7: Полезные трюки
```python

# Разные стили графиков
print(plt.style.available)  # Посмотреть доступные стили
plt.style.use('ggplot')  # Использовать стиль ggplot

# Математические формулы в тексте
plt.title(r'$\sigma = 15\ \mu=100$')  # Формулы в LaTeX

# Аннотации
plt.annotate('Пик значения', xy=(3, 17), xytext=(4, 18),
            arrowprops=dict(facecolor='black', shrink=0.05))

# Логарифмическая шкала
plt.yscale('log')
```

Что делать, если график не отображается:

    Не забывай plt.show() — особенно в скриптах

    В Jupyter используй %matplotlib inline — чтобы графики показывались в ноутбуке

    Проверь данные — нет ли пустых значений или нечисловых данных

    Убедись в импорте — import matplotlib.pyplot as plt
    
# RDKit

RDKit - главная штука для работы с химией, без нее жить тяжко ~с ней не меньше~

Шаг 1: Базовые понятия

    SMILES — текстовое представление молекулы (например, 'CCO' — это этанол)

    Mol объект — главный объект RDKit, с которым ты будешь работать

    Химические дескрипторы — числовые характеристики молекул

    Молекулярные отпечатки — бинарные векторы для сравнения молекул

Шаг 2: Чтение молекул (основа всего)
```python

from rdkit import Chem
from rdkit.Chem import Draw
from rdkit.Chem.Draw import IPythonConsole

# Из SMILES (самый простой способ)
smiles = 'CCO'  # Этанол
mol = Chem.MolFromSmiles(smiles)
mol

Если работаешь в Jupyter, увидишь картинку молекулы!

Другие форматы:
python

# Из SDF файла (структурный файл)
suppl = Chem.SDMolSupplier('molecules.sdf')
mols = [mol for mol in suppl if mol is not None]

# Из MOL файла
mol = Chem.MolFromMolFile('molecule.mol')

# Из названия вещества (магия!)
import urllib.request
def name_to_smiles(name):
    url = f"https://cactus.nci.nih.gov/chemical/structure/{name}/smiles"
    return urllib.request.urlopen(url).read().decode()

caffeine_smiles = name_to_smiles('caffeine')
caffeine = Chem.MolFromSmiles(caffeine_smiles)
caffeine
```

Шаг 3: Визуализация (рисуем молекулы)
```python

# Одна молекула
mol = Chem.MolFromSmiles('c1ccccc1O')  # Фенол
display(mol)

# Несколько молекул в сетке
mols = [Chem.MolFromSmiles(smiles) for smiles in ['CCO', 'CCOC', 'CCN']]
img = Draw.MolsToGridImage(mols, molsPerRow=3, subImgSize=(300, 300))
img

# С атомными индексами (чтобы видеть номера атомов)
def mol_with_atom_index(mol):
    for atom in mol.GetAtoms():
        atom.SetAtomMapNum(atom.GetIdx())
    return mol

display(mol_with_atom_index(mol))
```

Шаг 4: Основные операции с молекулами
```python

mol = Chem.MolFromSmiles('CCO')

# Базовая информация
print(f"Количество атомов: {mol.GetNumAtoms()}")
print(f"Количество связей: {mol.GetNumBonds()}")

# Информация об атомах
for atom in mol.GetAtoms():
    print(f"Атом {atom.GetIdx()}: {atom.GetSymbol()}, "
          f"водороды: {atom.GetTotalNumHs()}")

# Информация о связях
for bond in mol.GetBonds():
    print(f"Связь: {bond.GetBeginAtomIdx()}-{bond.GetEndAtomIdx()}, "
          f"тип: {bond.GetBondType()}")
```

Шаг 5: Поиск подструктур (химический поиск)
```python

# Ищем определенные группы атомов в молекуле
mol = Chem.MolFromSmiles('c1ccccc1O')  # Фенол
hydroxyl = Chem.MolFromSmarts('[OH]')  # Гидроксильная группа

# Проверяем, есть ли гидроксил в молекуле
if mol.HasSubstructMatch(hydroxyl):
    matches = mol.GetSubstructMatches(hydroxyl)
    print(f"Найдено гидроксильных групп: {len(matches)}")
    
    # Подсвечиваем найденные группы
    mol.__sssAtoms = [atom for match in matches for atom in match]
    display(mol)
```

Шаг 6: Молекулярные дескрипторы (превращаем в цифры)
```python

from rdkit.Chem import Descriptors
from rdkit.Chem import rdMolDescriptors

mol = Chem.MolFromSmiles('CCO')  # Этанол

# Базовые дескрипторы
print(f"Молекулярная масса: {Descriptors.MolWt(mol):.2f}")
print(f"LogP: {Descriptors.MolLogP(mol):.2f}")
print(f"Количество доноров H-связей: {Descriptors.NumHDonors(mol)}")
print(f"Количество акцепторов H-связей: {Descriptors.NumHAcceptors(mol)}")

# Более сложные дескрипторы
print(f"Полярная площадь поверхности: {Descriptors.TPSA(mol):.2f}")
print(f"Количество вращающихся связей: {Descriptors.NumRotatableBonds(mol)}")
```

Шаг 7: Молекулярные отпечатки (для машинного обучения)
```python

from rdkit.Chem import AllChem
from rdkit import DataStructs

# Создаем отпечатки пальцев для ML
mol = Chem.MolFromSmiles('CCO')
fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=1024)

# Превращаем в массив numpy для sklearn
fp_array = np.array(fp)
print(f"Размер вектора: {len(fp_array)}")
print(f"Первые 10 битов: {fp_array[:10]}")

# Сравниваем две молекулы
mol1 = Chem.MolFromSmiles('CCO')  # Этанол
mol2 = Chem.MolFromSmiles('CCCO')  # Пропанол

fp1 = AllChem.GetMorganFingerprintAsBitVect(mol1, 2, 1024)
fp2 = AllChem.GetMorganFingerprintAsBitVect(mol2, 2, 1024)

similarity = DataStructs.TanimotoSimilarity(fp1, fp2)
print(f"Схожесть молекул: {similarity:.3f}")
```

Шаг 8: Поиск максимальной общей подструктуры
```python

from rdkit.Chem import rdFMCS

# Находим общую часть для нескольких молекул
mol1 = Chem.MolFromSmiles('CCOC')   # Диэтиловый эфир
mol2 = Chem.MolFromSmiles('CCOCC')  # Дипропиловый эфир

mcs = rdFMCS.FindMCS([mol1, mol2])
common_core = Chem.MolFromSmarts(mcs.smartsString)
print(f"Общая подструктура: {mcs.smartsString}")

# Визуализируем
display(common_core)
```

Шаг 9: Работа с большими наборами данных
```python

import pandas as pd

# Создаем датафрейм с молекулами
data = {
    'name': ['Этанол', 'Метанол', 'Пропанол'],
    'smiles': ['CCO', 'CO', 'CCCO']
}
df = pd.DataFrame(data)

# Добавляем молекулы и дескрипторы
df['mol'] = df['smiles'].apply(Chem.MolFromSmiles)
df['mw'] = df['mol'].apply(Descriptors.MolWt)
df['logp'] = df['mol'].apply(Descriptors.MolLogP)
df['tpsa'] = df['mol'].apply(Descriptors.TPSA)

print(df[['name', 'mw', 'logp', 'tpsa']])
```

Что делать, если всё падает:

    Проверь версию Python — RDKit лучше работает с 3.7-3.9

    Используй conda — меньше проблем с зависимостями 

Для новых версий Python — используй rdkit-pypi

    Проверь SMILES — иногда проблема в неправильной химической формуле

Памятка начинающего химика-программиста:

    Все молекулы в RDKit живут как Mol объекты

    SMILES — твой лучший друг для быстрого создания молекул

    Всегда проверяй, что mol is not None после создания молекулы

    Используй молекулярные отпечатки для машинного обучения

    Химические дескрипторы — для традиционного QSAR моделирования

  # Словарь по Python для хим-информатики 

## Содержание
- Базовый Python
- Pandas
- NumPy
- Scikit-learn
- Matplotlib
- RDKit
- Химические термины

---

## Базовый Python

### Типы данных
```python
# Числовые
int_var = 42           # Целое число
float_var = 3.14       # Число с плавающей точкой
complex_var = 1 + 2j   # Комплексное число

# Строки
str_var = "Hello"      # Строка
str_multiline = """Многострочная
строка"""

# Булевы
bool_var = True        # Логический тип
```

### Коллекции
```python
# Список (изменяемый)
my_list = [1, 2, 3]
my_list.append(4)      # Добавить элемент
my_list.extend([5,6])  # Добавить несколько
my_list.insert(0, 0)   # Вставить по индексу
my_list.remove(3)      # Удалить элемент
my_list.pop()          # Удалить и вернуть последний
my_list.index(2)       # Найти индекс элемента
my_list.count(1)       # Посчитать вхождения
my_list.sort()         # Сортировка
my_list.reverse()      # Обратный порядок
my_list.copy()         # Копия списка
my_list.clear()        # Очистить список

# Кортеж (неизменяемый)
my_tuple = (1, 2, 3)
my_tuple.count(1)      # Количество вхождений
my_tuple.index(2)      # Индекс элемента

# Словарь (ключ-значение)
my_dict = {'a': 1, 'b': 2}
my_dict.keys()         # Все ключи
my_dict.values()       # Все значения  
my_dict.items()        # Пары ключ-значение
my_dict.get('a')       # Получить значение
my_dict.pop('a')       # Удалить и вернуть
my_dict.update({'c':3})# Обновить словарь
my_dict.clear()        # Очистить

# Множество (уникальные элементы)
my_set = {1, 2, 3}
my_set.add(4)          # Добавить элемент
my_set.remove(1)       # Удалить элемент
my_set.discard(5)      # Удалить если есть
my_set.union({4,5})    # Объединение
my_set.intersection({2,3}) # Пересечение
my_set.difference({1}) # Разность
```

### Управляющие конструкции
```python
# Условные операторы
if condition:
    # код
elif other_condition:
    # код  
else:
    # код

# Тернарный оператор
value = x if condition else y

# Циклы
for item in iterable:
    # код
    break    # Прервать цикл
    continue # Пропустить итерацию

while condition:
    # код

# Генераторы списков
[x*2 for x in range(5) if x > 2]
```

### Функции
```python
def function_name(param1, param2=default_value):
    """Docstring с описанием функции"""
    return result

# Аргументы переменной длины
def func(*args, **kwargs):
    # args - кортеж позиционных аргументов
    # kwargs - словарь именованных аргументов

# Лямбда-функции
lambda x: x**2

# Декораторы
@decorator
def func():
    pass
```

---

## Pandas

### Основные объекты
```python
import pandas as pd

# Series (одномерный массив)
s = pd.Series([1, 2, 3], index=['a', 'b', 'c'])

# DataFrame (двумерная таблица)
df = pd.DataFrame({
    'col1': [1, 2, 3],
    'col2': ['a', 'b', 'c']
})
```

### Чтение/запись данных
```python
# Чтение
df = pd.read_csv('file.csv')
df = pd.read_excel('file.xlsx')
df = pd.read_json('file.json')

# Запись
df.to_csv('file.csv', index=False)
df.to_excel('file.xlsx', index=False)
df.to_json('file.json')
```

### Просмотр и информация
```python
df.head(n)           # Первые n строк
df.tail(n)           # Последние n строк
df.shape             # Размерность (строки, столбцы)
df.info()            # Информация о данных
df.describe()        # Статистика числовых столбцов
df.dtypes            # Типы данных столбцов
df.columns           # Названия столбцов
df.index             # Индексы строк
```

### Выбор данных
```python
# Выбор столбцов
df['col_name']       # Один столбец (Series)
df[['col1', 'col2']] # Несколько столбцов (DataFrame)

# Выбор строк
df.loc[index]        # По метке индекса
df.iloc[position]    # По позиции

# Срезы
df.loc[start:stop]   # По меткам
df.iloc[start:stop]  # По позициям

# Булева индексация
df[df['col'] > 0]    # Фильтрация по условию
df.query('col > 0')  # SQL-подобный запрос
```

### Модификация данных
```python
# Добавление/удаление столбцов
df['new_col'] = values
df.drop('col', axis=1, inplace=True)

# Переименование
df.rename(columns={'old': 'new'}, inplace=True)

# Изменение индекса
df.set_index('col', inplace=True)
df.reset_index(inplace=True)

# Сортировка
df.sort_values('col', ascending=False)
df.sort_index()
```

### Обработка пропусков
```python
df.isnull()          # Поиск пропусков
df.notnull()         # Поиск непустых значений
df.dropna()          # Удаление строк с пропусками
df.fillna(value)     # Заполнение пропусков
```

### Группировка и агрегация
```python
# Группировка
grouped = df.groupby('col')

# Агрегатные функции
grouped.mean()       # Среднее
grouped.sum()        # Сумма
grouped.count()      # Количество
grouped.min()        # Минимум
grouped.max()        # Максимум
grouped.std()        # Стандартное отклонение

# Несколько агрегаций
grouped.agg({
    'col1': ['mean', 'sum'],
    'col2': 'count'
})
```

### Объединение данных
```python
# Конкатенация
pd.concat([df1, df2])

# Объединение как в SQL
pd.merge(df1, df2, on='key')
pd.merge(df1, df2, left_on='lkey', right_on='rkey')

# Join по индексу
df1.join(df2)
```

### Временные ряды
```python
# Преобразование в datetime
pd.to_datetime(df['date_col'])

# Работа с датами
df['date'].dt.year   # Год
df['date'].dt.month  # Месяц
df['date'].dt.day    # День
```

---

## NumPy

### Создание массивов
```python
import numpy as np

# Основные способы
np.array([1, 2, 3])           # Из списка
np.zeros(shape)               # Массив нулей
np.ones(shape)                # Массив единиц
np.empty(shape)               # Пустой массив
np.arange(start, stop, step)  # Аналог range
np.linspace(start, stop, num) # Равномерно распределенные
np.random.random(shape)       # Случайные числа [0, 1)
np.random.randn(shape)        # Нормальное распределение
```

### Свойства массивов
```python
arr.shape      # Форма массива
arr.ndim       # Количество измерений
arr.size       # Общее количество элементов
arr.dtype      # Тип данных
arr.itemsize   # Размер одного элемента в байтах
arr.nbytes     # Общий размер в байтах
```

### Индексация и срезы
```python
# Одномерные массивы
arr[0]         # Первый элемент
arr[-1]        # Последний элемент
arr[2:5]       # Срез
arr[::2]       # Каждый второй

# Многомерные массивы
arr2d[0, 1]    # Первая строка, второй столбец
arr2d[0:2, 1:3] # Подматрица

# Булева индексация
arr[arr > 0]   # Элементы больше 0
```

### Изменение формы
```python
arr.reshape(new_shape)  # Изменение формы
arr.flatten()           # В одномерный массив
arr.ravel()             # Плоское представление
arr.T                   # Транспонирование
arr.swapaxes(0, 1)      # Перестановка осей
```

### Математические операции
```python
# Арифметические
np.add(x, y)           # Сложение
np.subtract(x, y)      # Вычитание  
np.multiply(x, y)      # Умножение
np.divide(x, y)        # Деление
np.power(x, y)         # Возведение в степень
np.sqrt(x)             # Квадратный корень

# Тригонометрические
np.sin(x), np.cos(x), np.tan(x)
np.arcsin(x), np.arccos(x), np.arctan(x)

# Экспонента и логарифмы
np.exp(x)              # Экспонента
np.log(x)              # Натуральный логарифм
np.log10(x)            # Десятичный логарифм

# Округление
np.round(x)            # Округление
np.floor(x)            # Округление вниз
np.ceil(x)             # Округление вверх
```

### Статистические функции
```python
np.mean(arr)           # Среднее значение
np.median(arr)         # Медиана
np.std(arr)            # Стандартное отклонение
np.var(arr)            # Дисперсия
np.min(arr)            # Минимум
np.max(arr)            # Максимум
np.argmin(arr)         # Индекс минимума
np.argmax(arr)         # Индекс максимума
np.sum(arr)            # Сумма
np.prod(arr)           # Произведение
np.cumsum(arr)         # Кумулятивная сумма
np.cumprod(arr)        # Кумулятивное произведение
```

### Линейная алгебра
```python
np.dot(a, b)           # Скалярное произведение
np.matmul(a, b)        # Матричное умножение
np.linalg.inv(a)       # Обратная матрица
np.linalg.det(a)       # Определитель
np.linalg.eig(a)       # Собственные значения/векторы
np.linalg.solve(a, b)  # Решение системы уравнений
```

---

## Scikit-learn

### Основные модули
```python
from sklearn import datasets      # Наборы данных
from sklearn import preprocessing # Предобработка
from sklearn import model_selection # Разделение данных
from sklearn import metrics       # Метрики
from sklearn import linear_model  # Линейные модели
from sklearn import ensemble      # Ансамбли
from sklearn import svm           # Метод опорных векторов
from sklearn import neighbors     # Ближайшие соседи
from sklearn import tree          # Деревья решений
from sklearn import cluster       # Кластеризация
```

### Предобработка данных
```python
# Масштабирование
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_normalized = scaler.fit_transform(X)

# Кодирование категориальных признаков
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
y_encoded = encoder.fit_transform(y)

from sklearn.preprocessing import OneHotEncoder
encoder = OneHotEncoder()
X_encoded = encoder.fit_transform(X)

# Работа с пропусками
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)
```

### Разделение данных
```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Кросс-валидация
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5)

from sklearn.model_selection import KFold
kf = KFold(n_splits=5, shuffle=True, random_state=42)
```

### Модели классификации
```python
# Линейные модели
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()

# Метод опорных векторов
from sklearn.svm import SVC
model = SVC(kernel='rbf')

# Деревья решений
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier()

# Случайный лес
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100)

# Градиентный бустинг
from sklearn.ensemble import GradientBoostingClassifier
model = GradientBoostingClassifier()

# K-ближайших соседей
from sklearn.neighbors import KNeighborsClassifier
model = KNeighborsClassifier(n_neighbors=5)

# Наивный Байес
from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
```

### Модели регрессии
```python
# Линейная регрессия
from sklearn.linear_model import LinearRegression
model = LinearRegression()

# Гребневая регрессия
from sklearn.linear_model import Ridge
model = Ridge(alpha=1.0)

# Лассо регрессия
from sklearn.linear_model import Lasso
model = Lasso(alpha=1.0)

# Случайный лес для регрессии
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()

# Градиентный бустинг для регрессии
from sklearn.ensemble import GradientBoostingRegressor
model = GradientBoostingRegressor()

# Метод опорных векторов для регрессии
from sklearn.svm import SVR
model = SVR()
```

### Кластеризация
```python
# K-средних
from sklearn.cluster import KMeans
model = KMeans(n_clusters=3)

# Иерархическая кластеризация
from sklearn.cluster import AgglomerativeClustering
model = AgglomerativeClustering(n_clusters=3)

# DBSCAN
from sklearn.cluster import DBSCAN
model = DBSCAN(eps=0.5, min_samples=5)
```

### Метрики качества
```python
# Классификация
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_true, y_pred)

from sklearn.metrics import precision_score
precision = precision_score(y_true, y_pred)

from sklearn.metrics import recall_score
recall = recall_score(y_true, y_pred)

from sklearn.metrics import f1_score
f1 = f1_score(y_true, y_pred)

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_true, y_pred)

from sklearn.metrics import classification_report
report = classification_report(y_true, y_pred)

from sklearn.metrics import roc_auc_score
auc = roc_auc_score(y_true, y_pred)

# Регрессия
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_true, y_pred)

from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_true, y_pred)

from sklearn.metrics import r2_score
r2 = r2_score(y_true, y_pred)
```

### Подбор гиперпараметров
```python
# Сетчатый поиск
from sklearn.model_selection import GridSearchCV
param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
grid_search = GridSearchCV(SVC(), param_grid, cv=5)
grid_search.fit(X, y)

# Случайный поиск
from sklearn.model_selection import RandomizedSearchCV
param_dist = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
random_search = RandomizedSearchCV(SVC(), param_dist, n_iter=10, cv=5)
random_search.fit(X, y)
```

### Конвейеры
```python
from sklearn.pipeline import Pipeline
from sklearn.feature_selection import SelectKBest

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('selector', SelectKBest(k=10)),
    ('classifier', RandomForestClassifier())
])

pipeline.fit(X_train, y_train)
```

---

## Matplotlib

### Основные функции
```python
import matplotlib.pyplot as plt

# Создание фигуры и осей
fig, ax = plt.subplots(figsize=(10, 6))

# Линейный график
ax.plot(x, y, label='Линия', color='red', linewidth=2, linestyle='--')

# Точечный график
ax.scatter(x, y, s=50, c='blue', alpha=0.5, marker='o')

# Столбчатая диаграмма
ax.bar(categories, values, color='green', alpha=0.7)

# Гистограмма
ax.hist(data, bins=30, density=True, alpha=0.6, color='orange')

# Круговая диаграмма
ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)

# Настройки
ax.set_title('Заголовок')
ax.set_xlabel('Ось X')
ax.set_ylabel('Ось Y')
ax.legend()
ax.grid(True)

# Сохранение
plt.savefig('plot.png', dpi=300, bbox_inches='tight')
plt.show()
```

### Стили и оформление
```python
# Доступные стили
plt.style.available
plt.style.use('ggplot')

# Цвета
'red', 'blue', 'green', '#FF00FF', (0.1, 0.2, 0.5)

# Маркеры
'.', 'o', 's', '^', 'D', '*', '+'

# Стили линий
'-', '--', '-.', ':'

# Размеры и разрешение
plt.rcParams['figure.figsize'] = [10, 6]
plt.rcParams['figure.dpi'] = 100
```

### Специализированные графики
```python
# Box plot
plt.boxplot(data)

# Violin plot
plt.violinplot(data)

# Heatmap
plt.imshow(matrix, cmap='hot')
plt.colorbar()

# Contour plot
plt.contour(X, Y, Z)
plt.contourf(X, Y, Z)

# 3D графики
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(X, Y, Z)
```

---

## RDKit

### Основные объекты и функции
```python
from rdkit import Chem
from rdkit.Chem import AllChem
from rdkit.Chem import Draw
from rdkit.Chem import Descriptors
from rdkit.Chem import rdMolDescriptors

# Создание молекул
mol = Chem.MolFromSmiles('CCO')           # Из SMILES
mol = Chem.MolFromMolFile('file.mol')     # Из MOL файла
mol = Chem.MolFromMolBlock(mol_block)     # Из MOL блока
mol = Chem.MolFromSmarts('[OH]')          # Из SMARTS

# Обратное преобразование
smiles = Chem.MolToSmiles(mol)
mol_block = Chem.MolToMolBlock(mol)
```

### Операции с молекулами
```python
# Базовая информация
mol.GetNumAtoms()                    # Количество атомов
mol.GetNumBonds()                    # Количество связей
mol.GetAtomWithIdx(0).GetSymbol()    # Символ атома
mol.GetBondWithIdx(0).GetBondType()  # Тип связи

# Модификация
Chem.AddHs(mol)                      # Добавить водороды
Chem.RemoveHs(mol)                   # Удалить водороды
Chem.SanitizeMol(mol)                # Проверить валидность

# Подструктуры
mol.HasSubstructMatch(pattern)       # Проверить наличие
mol.GetSubstructMatches(pattern)     # Найти все вхождения
```

### Дескрипторы
```python
# Физико-химические свойства
Descriptors.MolWt(mol)               # Молекулярная масса
Descriptors.MolLogP(mol)             # LogP
Descriptors.NumHDonors(mol)          # Доноры H-связей
Descriptors.NumHAcceptors(mol)       # Акцепторы H-связей
Descriptors.TPSA(mol)                # Полярная площадь поверхности

# Структурные дескрипторы
Descriptors.NumRotatableBonds(mol)   # Вращающиеся связи
Descriptors.NumAromaticRings(mol)    # Ароматические кольца
Descriptors.FractionCsp3(mol)        # Доля sp3 гибридизации
```

### Молекулярные отпечатки
```python
# Morgan fingerprints (ECFP)
fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=1024)

# Topological fingerprints
fp = Chem.RDKFingerprint(mol)

# Atom pairs
fp = AllChem.GetAtomPairFingerprintAsBitVect(mol)

# Сходство
from rdkit import DataStructs
similarity = DataStructs.TanimotoSimilarity(fp1, fp2)
```

### Визуализация
```python
# Одна молекула
Draw.MolToImage(mol, size=(300, 300))

# Несколько молекул
Draw.MolsToGridImage(mols, molsPerRow=4, subImgSize=(200, 200))

# С подстветкой
Draw.MolToImage(mol, highlightAtoms=[0, 1, 2])

# В файл
Draw.MolToFile(mol, 'molecule.png')
```

### Химические преобразования
```python
# Стереохимия
Chem.AssignAtomChiralTagsFromStructure(mol)
Chem.AssignStereochemistry(mol)

# Выравнивание
AllChem.Compute2DCoords(mol)
AllChem.GenerateDepictionMatching2DStructure(mol, template)

# Конформеры
AllChem.EmbedMolecule(mol)
AllChem.MMFFOptimizeMolecule(mol)
```

### Работа с базами данных
```python
# SD файлы
suppl = Chem.SDMolSupplier('compounds.sdf')
mols = [mol for mol in suppl if mol is not None]

# Запись SD файла
writer = Chem.SDWriter('output.sdf')
for mol in mols:
    writer.write(mol)
writer.close()
```

---

## Химические термины

### Основные понятия
- **SMILES** - Simplified Molecular Input Line Entry System
- **SMARTS** - SMILES Arbitrary Target Specification  
- **InChI** - International Chemical Identifier
- **MOL file** - Файл формата MDL Molfile
- **SDF** - Structure Data File

### Физико-химические свойства
- **LogP** - Коэффициент распределения октанол/вода
- **TPSA** - Topological Polar Surface Area
- **HBA** - Hydrogen Bond Acceptors
- **HBD** - Hydrogen Bond Donors
- **MR** - Molar Refractivity

### Молекулярные дескрипторы
- **2D дескрипторы** - Рассчитываются из молекулярного графа
- **3D дескрипторы** - Требуют пространственной структуры
- **Фрагментные дескрипторы** - Основаны на наличии определенных групп
- **Целомолекулярные дескрипторы** - Описывают молекулу в целом

### Методы анализа
- **QSAR** - Quantitative Structure-Activity Relationship
- **QSPR** - Quantitative Structure-Property Relationship  
- **Molecular docking** - Молекулярный докинг
- **Virtual screening** - Виртуальный скрининг
- **ADMET** - Absorption, Distribution, Metabolism, Excretion, Toxicity

---







